{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm6vyAoo-Xoj"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input/kernel-methods-ammi-2023/Xtr0.csv'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "IdY5JH7L-egP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtr0=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2023/Xtr0.csv\")\n",
        "Ytr0=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2023/Ytr0.csv\")\n",
        "\n",
        "Xtr1=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2023/Xtr1.csv\")\n",
        "Ytr1=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2023/Ytr1.csv\")\n",
        "\n",
        "Xtr2=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2023/Xtr2.csv\")\n",
        "Ytr2=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2023/Ytr2.csv\")"
      ],
      "metadata": {
        "id": "2WAIC3dl-hXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xte0=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2023/Xte0.csv\")\n",
        "# Yte0=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2023/Yte0.csv\")\n",
        "\n",
        "Xte1=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2023/Xte1.csv\")\n",
        "# Yte1=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2023/Yte1.csv\")\n",
        "\n",
        "Xte2=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2023/Xte2.csv\")\n",
        "# Yte2=pd.read_csv(\"/kaggle/input/kernel-methods-ammi-2023/Yte2.csv\")"
      ],
      "metadata": {
        "id": "1OFTJBbR-hbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ytr0"
      ],
      "metadata": {
        "id": "h3FcgbMx-hei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ytr0.Bound.hist()"
      ],
      "metadata": {
        "id": "A2gx6ysN-o7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ytr1.Bound.hist()"
      ],
      "metadata": {
        "id": "Pgmi7RKl-o_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ytr2.Bound.hist()"
      ],
      "metadata": {
        "id": "R9UUC7J8-hhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataTr0=pd.concat([Xtr0, Ytr0[[\"Bound\"]]], axis=1, join='inner')\n",
        "dataTr1=pd.concat([Xtr1, Ytr1[[\"Bound\"]]], axis=1, join='inner')\n",
        "dataTr2=pd.concat([Xtr2, Ytr2[[\"Bound\"]]], axis=1)#, join='inner')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VPkFcmig-hkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def one_hot_encode(seq):\n",
        "    \"\"\"\n",
        "    Given a DNA sequence, return its one-hot encoding\n",
        "    \"\"\"\n",
        "    # Make sure seq has only allowed bases\n",
        "    allowed = set(\"ACTGN\")\n",
        "    if not set(seq).issubset(allowed):\n",
        "        invalid = set(seq) - allowed\n",
        "        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n",
        "\n",
        "    # Dictionary returning one-hot encoding for each nucleotide\n",
        "    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n",
        "             'C':[0.0,1.0,0.0,0.0],\n",
        "             'G':[0.0,0.0,1.0,0.0],\n",
        "             'T':[0.0,0.0,0.0,1.0],\n",
        "             'N':[0.0,0.0,0.0,0.0]}\n",
        "\n",
        "    # Create array from nucleotide sequence\n",
        "    vec=np.array([nuc_d[x] for x in seq])\n",
        "\n",
        "    return vec"
      ],
      "metadata": {
        "id": "fXQwPGJY-0yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OneHot(df,seq_col='seq',target_col=None):\n",
        "        # +--------------------+\n",
        "        # | Get the X examples |\n",
        "        # +--------------------+\n",
        "        # extract the DNA from the appropriate column in the df\n",
        "        seqs = list(df[seq_col].values)\n",
        "        seq_len = len(seqs[0])\n",
        "\n",
        "        # one-hot encode sequences, then stack in a torch tensor\n",
        "         #self.ohe_seqs = torch.stack([torch.tensor(one_hot_encode(x) for x in self.seqs])\n",
        "        ohe_seqs = np.stack([one_hot_encode(x).flatten() for x in seqs])\n",
        "        n,p= ohe_seqs.shape\n",
        "        #         self.ohe_seqs=self.add_ones(self.ohe_seqs)\n",
        "        print(n,p)\n",
        "        if target_col is not None:\n",
        "            labels = (df[target_col].values)\n",
        "            print( labels.shape)\n",
        "            return ohe_seqs ,labels\n",
        "        return ohe_seqs\n"
      ],
      "metadata": {
        "id": "rTgigpPS-01b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_0,y_train_0 = OneHot(dataTr0,target_col=\"Bound\")\n",
        "\n",
        "x_train_1,y_train_1 = OneHot(dataTr1,target_col=\"Bound\")\n",
        "\n",
        "x_train_2,y_train_2 = OneHot(dataTr2,target_col=\"Bound\")\n",
        "\n",
        "Xte0=OneHot(Xte0)\n",
        "Xte1=OneHot(Xte1)\n",
        "Xte2=OneHot(Xte2)"
      ],
      "metadata": {
        "id": "cmd0Cz-Z-036"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    '''\n",
        "    The goal of this class is to create a LogisticRegression class,\n",
        "    that we will use as our model to classify data point into a corresponding class\n",
        "    '''\n",
        "    def __init__(self,lam,lr,n_epochs,eps=1e-12):\n",
        "        self.lr = lr\n",
        "        self.lam =lam\n",
        "        self.eps=eps\n",
        "        self.n_epochs = n_epochs\n",
        "        self.train_losses = []\n",
        "        self.w = None\n",
        "        self.weight = []\n",
        "    def add_ones(self, x):\n",
        "        ##### WRITE YOUR CODE HERE #####\n",
        "        return np.hstack((np.ones((x.shape[0],1)),x))\n",
        "        #pass\n",
        "        #### END CODE ####\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        ##### WRITE YOUR CODE HERE ####\n",
        "        return 1/(1+np.exp(-x@self.w))\n",
        "        #pass\n",
        "        #### END CODE ####\n",
        "\n",
        "    def cross_entropy(self, x, y_true):\n",
        "        ##### WRITE YOUR CODE HERE #####\n",
        "        y_pred = self.sigmoid(x)\n",
        "        loss =-np.sum( y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))/x.shape[0]\n",
        "        return loss\n",
        "        #### END CODE ####\n",
        "\n",
        "    def predict_proba(self,x):  #This function will use the sigmoid function to compute the probalities\n",
        "        ##### WRITE YOUR CODE HERE #####\n",
        "        #x= self.add_ones(x)\n",
        "        proba = self.sigmoid(x)\n",
        "        return proba\n",
        "        #### END CODE ####\n",
        "\n",
        "    def predict(self,x):\n",
        "        ##### WRITE YOUR CODE HERE #####\n",
        "        probas = self.predict_proba(self.add_ones(x))\n",
        "        output=np.zeros(probas.shape[0])\n",
        "        for i in range(probas.shape[0]):\n",
        "            if probas[i]>=0.5: #convert the probalities into 0 and 1 by using a treshold=0.5\n",
        "                output[i]=1\n",
        "            else:\n",
        "                output[i]=0\n",
        "\n",
        "\n",
        "        return output\n",
        "      #### END CODE ####\n",
        "\n",
        "    def fit(self,x,y):\n",
        "        # Add ones to x\n",
        "        x=self.add_ones(x)\n",
        "\n",
        "\n",
        "        # reshape y if needed\n",
        "        if y.ndim==1:\n",
        "             y=y.reshape(-1,1)\n",
        "\n",
        "        # Initialize w to zeros vector >>> (x.shape[1])\n",
        "        self.w=np.zeros((x.shape[1],1))\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "\n",
        "              # make predictions\n",
        "            self.ypred = self.sigmoid(x)\n",
        "\n",
        "              #compute the gradient\n",
        "            dl = (-1/y.shape[1])*x.T@(y-self.ypred.reshape(-1,1))+2*self.lam*self.w\n",
        "#             if np.mean((self.lam*dl)**2)<self.eps:\n",
        "#                 break\n",
        "              #update rule\n",
        "            self.w=self.w- self.lr*dl\n",
        "\n",
        "              #Compute and append the training loss in a list\n",
        "            loss = self.cross_entropy(x, y)\n",
        "            #print(x.shape)\n",
        "            self.train_losses.append(loss)\n",
        "\n",
        "            if epoch%100 == 0:\n",
        "                print(f'loss for epoch {epoch}  : {loss}')\n",
        "\n",
        "\n",
        "    def accuracy(self,y_true, y_pred):\n",
        "        ##### WRITE YOUR CODE HERE #####\n",
        "        acc = 0\n",
        "        for i in range(y_true.shape[0]):\n",
        "            if y_true[i]==y_pred[i]:\n",
        "                acc+=1\n",
        "        return (acc/y_true.shape[0])*100\n",
        "        #### END CODE ####"
      ],
      "metadata": {
        "id": "33PZs_w5-06H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-----------Training Xtr0-------------\")\n",
        "model = LogisticRegression(lam=0.1,lr=0.0001,n_epochs=20000,eps=1e-12)\n",
        "model.fit(x_train_0,y_train_0)\n",
        "Yte0=model.predict(Xte0)\n",
        "print(\"-----------Training Xtr1-------------\")\n",
        "#model.accuracy(y_test, y_pred)\n",
        "model = LogisticRegression(lam=0.1,lr=0.0001,n_epochs=20000,eps=1e-12)\n",
        "model.fit(x_train_1,y_train_1)\n",
        "Yte1=model.predict(Xte1)\n",
        "print(\"-----------Training Xtr2-------------\")\n",
        "model = LogisticRegression(lam=0.1,lr=0.0001,n_epochs=20000,eps=1e-12)\n",
        "model.fit(x_train_2,y_train_2)\n",
        "Yte2=model.predict(Xte2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XVdUBcwY-08w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Yte0=np.asarray(Yte0, dtype = 'int')\n",
        "Yte1=np.asarray(Yte1, dtype = 'int')\n",
        "Yte2=np.asarray(Yte2, dtype = 'int')\n",
        "Yte=np.concatenate(((Yte0,Yte1,Yte2)))\n",
        "Id=np.arange(len(Yte))\n",
        "Yte = pd.DataFrame(zip(Id,Yte),columns=[\"Id\",\"Bound\"]).set_index('Id')"
      ],
      "metadata": {
        "id": "Jeo607Vt-0-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Yte.to_csv(\"Yte.csv\")"
      ],
      "metadata": {
        "id": "T6KLV2_y-1BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2gnXRzF3-1Dx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}